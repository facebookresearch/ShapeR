{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ShapeR Evaluation Data Exploration\n",
        "\n",
        "This notebook explores the data format of the pickles for the ShapeR Evaluation Dataset.\n",
        "\n",
        "We'll examine:\n",
        "1. Data structure for the pickle files containing the samples\n",
        "2. Point cloud data\n",
        "3. Image data\n",
        "4. Object point projections\n",
        "5. Camera poses and camera intrinsics\n",
        "6. Ground-truth mesh\n",
        "7. Captions\n",
        "8. Dataset visualization for both RGB and non-RGB (SLAM) variants\n",
        "9. Dataloader usage example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Pickle File Structure Reference\n",
        "\n",
        "The following tables describe the structure of the pickle files containing a sample's data. All samples share the same structure.\n",
        "\n",
        "**Variable Definitions:**\n",
        "- $N$ = number of semi-dense 3D points from SLAM\n",
        "- $V$ = number of mesh vertices\n",
        "- $F$ = number of mesh faces (triangles)\n",
        "- $I_s$ = number of SLAM camera images\n",
        "- $I_r$ = number of RGB camera images\n",
        "- $P_i^s$ = number of points visible in SLAM image $i$ (varies per image)\n",
        "- $P_i^r$ = number of points visible in RGB image $i$ (varies per image)\n",
        "\n",
        "### Point Cloud and Bounding Box Data\n",
        "\n",
        "| Key | Description | Dimensions |\n",
        "|-----|-------------|------------|\n",
        "| `points_model` | Semi-dense 3D point cloud from SLAM reconstruction, in object/model coordinate frame | $(N, 3)$ |\n",
        "| `bounds` | Axis-aligned bounding box half-extents of the object | $(3,)$ |\n",
        "| `T_model_world` | Transformation from world frame to model/object frame, `T_zup_obj` is not used. <br/> Refer to `dataset.shaper_dataset.InferenceDataset.rescale_back` for usage | $(4, 4)$ |\n",
        "| `inv_dist_std` and `dist_std` | Placeholder for point quality metric (unused, zeroed out in this dataset) | $(N,)$ |\n",
        "\n",
        "### SLAM Camera Data\n",
        "\n",
        "| Key | Description | Dimensions |\n",
        "|-----|-------------|------------|\n",
        "| `image_data` | JPEG-encoded grayscale images from SLAM camera | list[$I_s$] of bytes |\n",
        "| `Ts_camera_model` | Transformation from model/object frame to SLAM camera frame (camera extrinsics) | $(I_s, 4, 4)$ |\n",
        "| `camera_params` | SLAM camera intrinsics (FISHEYE624 distortion model, 15 parameters) | $(I_s, 15)$ |\n",
        "| `object_point_projections` | 2D pixel coordinates of visible 3D points projected onto each SLAM image | list[$I_s$], each $(P_i^s, 2)$ |\n",
        "| `visible_points_model` | 3D coordinates of points visible in each SLAM image | list[$I_s$], each $(P_i^s, 3)$ |\n",
        "\n",
        "### RGB Camera Data\n",
        "\n",
        "| Key | Description | Dimensions |\n",
        "|-----|-------------|------------|\n",
        "| `rgb_image_data` | JPEG-encoded RGB images from RGB camera | list[$I_r$] of bytes |\n",
        "| `Ts_rgbCamera_model` | Transformation from model/object frame to RGB camera frame | $(I_r, 4, 4)$ |\n",
        "| `rgb_camera_params` | RGB camera intrinsics (FISHEYE624 distortion model, 15 parameters) | $(I_r, 15)$ |\n",
        "| `rgb_object_point_projections` | 2D pixel coordinates of points projected onto each RGB image | list[$I_r$], each $(P_i^r, 2)$ |\n",
        "| `rgb_visible_points_model` | 3D coordinates of points visible in each RGB image | list[$I_r$], each $(P_i^r, 3)$ |\n",
        "\n",
        "### Ground Truth Mesh\n",
        "\n",
        "| Key | Description | Dimensions |\n",
        "|-----|-------------|------------|\n",
        "| `mesh_vertices` | Ground truth mesh vertex positions | $(V, 3)$ |\n",
        "| `mesh_faces` | Ground truth mesh face indices (triangles) | $(F, 3)$ |\n",
        "\n",
        "### Other\n",
        "\n",
        "| Key | Description | Dimensions |\n",
        "|-----|-------------|------------|\n",
        "| `caption` | Text description: category + shape description. Falls back to `category` if not present | str |\n",
        "| `is_ariagen2` | Device flag: `True` = Aria Gen2, `False` = Aria Gen1 | bool |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Point Cloud Data\n",
        "\n",
        "The point cloud is stored in `points_model`. The points are zero centered, but in metric world scale. To place them in their world position use, refer to how `T_model_world` is used in `dataset.shaper_dataset.InferenceDataset.rescale_back`. Lets check out the point clouds for one of the samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import io\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from dataset.download import setup_data\n",
        "\n",
        "# Set default figure size\n",
        "plt.rcParams['figure.figsize'] = [12, 8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Default pickle file (same as infer_shape.py default)\n",
        "PKL_NAME = \"BNB2909__pitcher.pkl\"\n",
        "PKL_PATH = f\"data/{PKL_NAME}\"\n",
        "\n",
        "setup_data(PKL_NAME)\n",
        "\n",
        "# Load the pickle file\n",
        "with open(PKL_PATH, \"rb\") as f:\n",
        "    pkl_sample = pickle.load(f)\n",
        "\n",
        "# Extract point cloud\n",
        "points = pkl_sample[\"points_model\"].numpy()\n",
        "bounds = pkl_sample[\"bounds\"].numpy()\n",
        "\n",
        "# Quality metrics (lower is better for filtering)\n",
        "inv_dist_std = pkl_sample[\"inv_dist_std\"].numpy()  # theta\n",
        "dist_std = pkl_sample[\"dist_std\"].numpy()  # phi\n",
        "\n",
        "print(f\"Point cloud shape: {points.shape}\")\n",
        "print(f\"Bounds: {bounds}\")\n",
        "print(f\"Point range: [{points.min():.3f}, {points.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive point cloud visualization using plotly\n",
        "\n",
        "# Subsample for performance\n",
        "idx = np.random.choice(len(points), min(5000, len(points)), replace=False)\n",
        "pts = points[idx]\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=1,\n",
        "    specs=[[{'type': 'scatter3d'}]],\n",
        "    subplot_titles=('Point Cloud',)\n",
        ")\n",
        "\n",
        "# Full point cloud\n",
        "fig.add_trace(\n",
        "    go.Scatter3d(\n",
        "        x=pts[:, 0], y=pts[:, 1], z=pts[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=2, color='blue', opacity=0.6),\n",
        "        name='Points'\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f\"Point Cloud Visualization ({len(points)} points, showing {len(idx)} subsampled)\",\n",
        "    height=500,\n",
        "    showlegend=False,\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Image Data (SLAM Views)\n",
        "\n",
        "Images are stored as encoded bytes (JPEG/PNG). This contains all the images where the sample object was seen in the sequence. Let's decode and visualize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_data = pkl_sample[\"image_data\"]\n",
        "print(f\"Number of SLAM images: {len(image_data)}\")\n",
        "\n",
        "# Decode first image to check format\n",
        "first_img = Image.open(io.BytesIO(image_data[0]))\n",
        "print(f\"Image mode: {first_img.mode}\")\n",
        "print(f\"Image size: {first_img.size}\")\n",
        "\n",
        "rgb_image_data = pkl_sample[\"rgb_image_data\"]\n",
        "print(f\"\\nNumber of RGB images: {len(rgb_image_data)}\")\n",
        "first_rgb = Image.open(io.BytesIO(rgb_image_data[0]))\n",
        "print(f\"RGB Image mode: {first_rgb.mode}\")\n",
        "print(f\"RGB Image size: {first_rgb.size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize a grid of images\n",
        "n_show = min(16, len(image_data))\n",
        "cols = 4\n",
        "rows = (n_show + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(12, 3*rows))\n",
        "axes = axes.flatten() if n_show > 1 else [axes]\n",
        "\n",
        "selected_random_image_indices = random.sample(range(len(image_data)), n_show)\n",
        "for ax_idx, im_idx in enumerate(selected_random_image_indices):\n",
        "    img = Image.open(io.BytesIO(image_data[im_idx])).convert('L')\n",
        "    axes[ax_idx].imshow(np.array(img), cmap='gray')\n",
        "    axes[ax_idx].set_title(f\"View {im_idx}\")\n",
        "    axes[ax_idx].axis('off')\n",
        "\n",
        "# Hide unused axes\n",
        "for i in range(n_show, len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle(\"SLAM Grayscale Images\", fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Object Point Projections\n",
        "\n",
        "2D projections of 3D points onto each image view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "projections = pkl_sample[\"object_point_projections\"]\n",
        "print(f\"Number of projection sets: {len(projections)}\")\n",
        "print(f\"First projection shape: {projections[0].shape}\")\n",
        "\n",
        "# Visualize projections on images\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(12, 3*rows))\n",
        "axes = axes.flatten() if n_show > 1 else [axes]\n",
        "for ax_idx, im_idx in enumerate(selected_random_image_indices):\n",
        "    img = Image.open(io.BytesIO(image_data[im_idx])).convert('L')\n",
        "    axes[ax_idx].imshow(np.array(img), cmap='gray')\n",
        "\n",
        "    # Plot projections\n",
        "    uv = projections[im_idx].numpy()\n",
        "    axes[ax_idx].scatter(uv[:, 0], uv[:, 1], s=1, c='lime', alpha=0.5)\n",
        "    axes[ax_idx].set_title(f\"View {ax_idx}: {len(uv)} points\")\n",
        "    axes[ax_idx].axis('off')\n",
        "\n",
        "plt.suptitle(\"Point Projections on Images\", fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Camera Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Camera extrinsics (world-to-camera transforms)\n",
        "Ts_camera = pkl_sample[\"Ts_camera_model\"]\n",
        "print(f\"Camera extrinsics shape: {Ts_camera.shape}\")\n",
        "print(f\"Number of camera poses: {len(Ts_camera)}\")\n",
        "\n",
        "# Extract camera centers (inverse of extrinsics)\n",
        "camera_centers = []\n",
        "for i in range(len(Ts_camera)):\n",
        "    T_inv = np.linalg.inv(Ts_camera[i].numpy())\n",
        "    camera_centers.append(T_inv[:3, 3])\n",
        "camera_centers = np.array(camera_centers)\n",
        "\n",
        "print(f\"\\nCamera center range:\")\n",
        "print(f\"  X: [{camera_centers[:, 0].min():.3f}, {camera_centers[:, 0].max():.3f}]\")\n",
        "print(f\"  Y: [{camera_centers[:, 1].min():.3f}, {camera_centers[:, 1].max():.3f}]\")\n",
        "print(f\"  Z: [{camera_centers[:, 2].min():.3f}, {camera_centers[:, 2].max():.3f}]\")\n",
        "\n",
        "camera_params = pkl_sample[\"camera_params\"]\n",
        "print(f\"\\nCamera intrinsics: {len(camera_params)} matrices\")\n",
        "print(f\"First intrinsic matrix:\\n{camera_params[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Subsample point cloud\n",
        "idx = np.random.choice(len(points), min(2000, len(points)), replace=False)\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot point cloud\n",
        "fig.add_trace(\n",
        "  go.Scatter3d(\n",
        "      x=points[idx, 0], y=points[idx, 1], z=points[idx, 2],\n",
        "      mode='markers',\n",
        "      marker=dict(size=1, color='blue', opacity=0.3),\n",
        "      name='Point Cloud'\n",
        "  )\n",
        ")\n",
        "\n",
        "# Plot camera positions\n",
        "fig.add_trace(\n",
        "  go.Scatter3d(\n",
        "      x=camera_centers[:, 0], y=camera_centers[:, 1], z=camera_centers[:, 2],\n",
        "      mode='markers',\n",
        "      marker=dict(size=3, color='red', symbol='diamond'),\n",
        "      name='Camera Positions'\n",
        "  )\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "  title=\"Camera Positions Around Object\",\n",
        "  scene=dict(\n",
        "      xaxis_title='X',\n",
        "      yaxis_title='Y',\n",
        "      zaxis_title='Z',\n",
        "      aspectmode='data',  # Equal aspect ratio based on data ranges\n",
        "  ),\n",
        "  height=600,\n",
        "  legend=dict(x=0.8, y=0.9),\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ground Truth Mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh_verts = pkl_sample[\"mesh_vertices\"].numpy()\n",
        "mesh_faces = pkl_sample[\"mesh_faces\"].numpy()\n",
        "\n",
        "print(f\"Mesh vertices: {mesh_verts.shape}\")\n",
        "print(f\"Mesh faces: {mesh_faces.shape}\")\n",
        "print(f\"Vertex range: [{mesh_verts.min():.3f}, {mesh_verts.max():.3f}]\")\n",
        "\n",
        "# Interactive mesh visualization\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "  go.Mesh3d(\n",
        "      x=mesh_verts[:, 0],\n",
        "      y=mesh_verts[:, 1],\n",
        "      z=mesh_verts[:, 2],\n",
        "      i=mesh_faces[:, 0],\n",
        "      j=mesh_faces[:, 1],\n",
        "      k=mesh_faces[:, 2],\n",
        "      color='gray',\n",
        "      opacity=1.0,\n",
        "      flatshading=False,\n",
        "      lighting=dict(ambient=0.5, diffuse=0.8, specular=0.2),\n",
        "      lightposition=dict(x=100, y=200, z=300),\n",
        "      name='Ground Truth Mesh'\n",
        "  )\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "  title=f\"Ground Truth Mesh ({len(mesh_verts)} vertices, {len(mesh_faces)} faces)\",\n",
        "  scene=dict(\n",
        "      xaxis_title='X',\n",
        "      yaxis_title='Y',\n",
        "      zaxis_title='Z',\n",
        "      aspectmode='data',\n",
        "  ),\n",
        "  height=700,\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Caption / Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"caption\" in pkl_sample:\n",
        "    print(f\"Caption: {pkl_sample['caption']}\")\n",
        "elif \"category\" in pkl_sample:\n",
        "    print(f\"Category: {pkl_sample['category']}\")\n",
        "else:\n",
        "    print(\"No caption or category found\")\n",
        "\n",
        "# Check for device type flag\n",
        "print(f\"\\nis_ariagen2: {pkl_sample['is_ariagen2']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Using the InferenceDataset\n",
        "\n",
        "Now let's see how the  dataloader processes this data. This does a lot of heavy lifting for object centric reconstruction, such as selecting the views using a particular heuristic, cropping the image around the object, packing everything needed to perform reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import omegaconf\n",
        "from dataset.shaper_dataset import InferenceDataset\n",
        "\n",
        "# Load config\n",
        "config = omegaconf.OmegaConf.load(\"checkpoints/config.yaml\")\n",
        "\n",
        "# Create dataset with different view counts\n",
        "num_views = 4  # Can be 4, 8, 16\n",
        "\n",
        "dataset = InferenceDataset(\n",
        "    config,\n",
        "    paths=[PKL_PATH],\n",
        "    override_num_views=num_views,\n",
        ")\n",
        "\n",
        "# Get a sample\n",
        "sample = dataset[0]\n",
        "\n",
        "print(f\"Sample keys: {list(sample.keys())}\")\n",
        "print(f\"\\nSample details:\")\n",
        "for key, value in sample.items():\n",
        "    if isinstance(value, np.ndarray):\n",
        "        print(f\"  {key}: np.ndarray, shape={value.shape}, dtype={value.dtype}\")\n",
        "    elif isinstance(value, str):\n",
        "        print(f\"  {key}: '{value[:50]}...'\" if len(value) > 50 else f\"  {key}: '{value}'\")\n",
        "    else:\n",
        "        print(f\"  {key}: {type(value).__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Visualize Dataloader Output (Non-RGB / SLAM Variant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize processed images from dataloader\n",
        "images = sample[\"images\"]  # Shape: (N, C, H, W)\n",
        "masks = sample.get(\"masks_ingest\", None)\n",
        "\n",
        "n_images = len(images)\n",
        "cols = min(4, n_images)\n",
        "rows = (n_images + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
        "axes = axes.flatten() if n_images > 1 else [axes]\n",
        "\n",
        "for i in range(n_images):\n",
        "    # Images are (C, H, W), transpose to (H, W, C)\n",
        "    img = images[i].transpose(1, 2, 0)\n",
        "    if img.shape[2] == 1:\n",
        "        img = img[:, :, 0]\n",
        "        axes[i].imshow(img, cmap='gray')\n",
        "    else:\n",
        "        axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"Processed View {i}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "for i in range(n_images, len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle(f\"Dataloader Output: {n_images} Views (SLAM/Non-RGB)\", fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show masks\n",
        "if masks is not None:\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
        "    axes = axes.flatten() if n_images > 1 else [axes]\n",
        "\n",
        "    for i in range(n_images):\n",
        "        axes[i].imshow(masks[i], cmap='gray')\n",
        "        axes[i].set_title(f\"Mask {i}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    for i in range(n_images, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(\"Point Projection Masks\", fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rotation here is intentional, ShapeR model was trained on Aria gen 1 simulated images, which are rotated like this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize processed point cloud\n",
        "processed_points = sample[\"semi_dense_points\"]\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "idx = np.random.choice(len(processed_points), min(5000, len(processed_points)), replace=False)\n",
        "ax.scatter(processed_points[idx, 0], processed_points[idx, 1], processed_points[idx, 2], s=2, alpha=0.5)\n",
        "\n",
        "ax.set_title(f\"Processed Point Cloud (Filtered & Scaled)\\n{len(processed_points)} points\")\n",
        "ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
        "ax.set_xlim(-1, 1); ax.set_ylim(-1, 1); ax.set_zlim(-1, 1)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Scale factor: {sample['scale']:.4f}\")\n",
        "print(f\"Caption: {sample['caption']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Visualize Dataloader Output (RGB Variant)\n",
        "\n",
        "If RGB data is available, we can switch the image processor to use RGB images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if RGB data is available\n",
        "# To use RGB, we need to modify the image processor call\n",
        "# The key difference is is_rgb=True in get_image_data_based_on_strategy\n",
        "from dataset.image_processor import get_image_data_based_on_strategy, crop_pad_preselected_views_with_background\n",
        "\n",
        "scale = 0.9 / np.max(pkl_sample[\"bounds\"].numpy())\n",
        "\n",
        "# Get RGB images\n",
        "(\n",
        "    rectified_images_rgb,\n",
        "    rectified_point_masks_rgb,\n",
        "    rectified_camera_params_rgb,\n",
        "    selected_view_ext_rgb,\n",
        ") = get_image_data_based_on_strategy(\n",
        "    pkl_sample,\n",
        "    num_views=16,\n",
        "    scale=scale,\n",
        "    is_rgb=True,  # KEY DIFFERENCE: Use RGB\n",
        "    strategy=\"cluster\",\n",
        ")\n",
        "\n",
        "(\n",
        "    selected_view_imgs_rgb,\n",
        "    _,\n",
        "    rectified_masks_rgb,\n",
        "    _,\n",
        ") = crop_pad_preselected_views_with_background(\n",
        "    rectified_images_rgb,\n",
        "    rectified_point_masks_rgb,\n",
        "    rectified_camera_params_rgb,\n",
        "    config.encoder.dino_image_size,\n",
        "    add_point_locations=False,\n",
        ")\n",
        "\n",
        "# Visualize RGB images\n",
        "n_rgb = len(selected_view_imgs_rgb)\n",
        "fig, axes = plt.subplots(1, n_rgb, figsize=(4*n_rgb, 4))\n",
        "if n_rgb == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i in range(n_rgb):\n",
        "    img = selected_view_imgs_rgb[i].transpose(1, 2, 0)\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"RGB View {i}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle(\"RGB Variant Images\", fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that semi-dense point tracking was performed on SLAM images only. While SLAM images have reliable object visibility information, object's visibility in the RGB image is estimated via heuristics and there's a very small chance that the frames do not have the object in view, due to occluders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Using the DataLoader with Collate Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataLoader with the custom collate function\n",
        "# The custom_collate handles:\n",
        "# 1. Point cloud preprocessing into a SparseTensor for efficient 3D convolutions\n",
        "# 2. Keeping vertices/faces as lists (variable size per sample)\n",
        "# 3. Standard batching for everything else\n",
        "\n",
        "inference_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,  # Can increase for multiple samples\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=dataset.custom_collate,  # KEY: Use custom collate function\n",
        ")\n",
        "\n",
        "# Get a batch\n",
        "batch = next(iter(inference_loader))\n",
        "\n",
        "print(\"Batch keys and types:\")\n",
        "print(\"=\" * 60)\n",
        "for key, value in batch.items():\n",
        "    if hasattr(value, 'shape'):\n",
        "        print(f\"  {key}: {type(value).__name__}, shape={tuple(value.shape)}\")\n",
        "    elif isinstance(value, list):\n",
        "        if len(value) > 0 and hasattr(value[0], 'shape'):\n",
        "            print(f\"  {key}: list[{len(value)}] of {type(value[0]).__name__}, first shape={value[0].shape}\")\n",
        "        else:\n",
        "            print(f\"  {key}: list[{len(value)}]\")\n",
        "    else:\n",
        "        print(f\"  {key}: {type(value).__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The semi_dense_points is now a SparseTensor (from torchsparse)\n",
        "# This is an efficient representation for 3D point data used by sparse convolutions\n",
        "sparse_points = batch[\"semi_dense_points\"]\n",
        "print(\"SparseTensor structure:\")\n",
        "print(f\"  Coordinates (C): {sparse_points.C.shape} - quantized 3D coordinates + batch index\")\n",
        "print(f\"  Features (F): {sparse_points.F.shape} - point features (original xyz coordinates)\")\n",
        "\n",
        "# Move batch to device (GPU if available) with optional dtype conversion\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch = InferenceDataset.move_batch_to_device(batch, device, dtype=torch.bfloat16)\n",
        "\n",
        "print(f\"\\nBatch moved to: {device}\")\n",
        "print(f\"Images dtype: {batch['images'].dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Points about the DataLoader\n",
        "\n",
        "1. **Custom Collate Function**: The `custom_collate` function is essential because:\n",
        "   - Point clouds are converted to `torchsparse.SparseTensor` for efficient sparse 3D convolutions\n",
        "   - Mesh vertices/faces are kept as lists since they have variable sizes across samples\n",
        "   \n",
        "2. **SparseTensor**: The point cloud is quantized into discrete bins and stored as a sparse tensor with:\n",
        "   - `C` (coordinates): Quantized (x, y, z, batch_idx) integers\n",
        "   - `F` (features): Original continuous xyz coordinates\n",
        "   \n",
        "3. **Device Transfer**: Use `InferenceDataset.move_batch_to_device()` to properly move all tensors (including SparseTensors) to GPU with optional dtype conversion (e.g., bfloat16 for memory efficiency)\n",
        "\n",
        "4. **View Selection Strategies**: The dataset supports different strategies for selecting which views to use:\n",
        "   - `cluster`: K-means clustering on camera positions for diverse viewpoints (default)\n",
        "   - `last_n`: Last N views in the capture sequence\n",
        "   - `view_angle`: Hemisphere-based selection for even angular coverage"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "4101ef5e-8326-4cf3-b0a2-74ac58a418cd",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
